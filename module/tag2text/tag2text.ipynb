{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " * The Tag2Text Model\n",
    " * Written by Xinyu Huang\n",
    " * Edited by Jungwook Seo\n",
    "'''\n",
    "\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import gc\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from PIL import Image\n",
    "from ram.models import tag2text\n",
    "from ram import inference_tag2text as inference\n",
    "from ram import get_transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "        \n",
    "class Bias2Tag():\n",
    "    def __init__(self, \n",
    "                 gpu_num: int, \n",
    "                 dataset: str, \n",
    "                 class_name: dict[str: str],\n",
    "                 conflict_ratio: str, \n",
    "                 root_path: str,\n",
    "                 pretrained_path: str,\n",
    "                 tag2text_thres: float,\n",
    "                 image_size=224):\n",
    "        self.root_path = root_path\n",
    "        self.conflict_ratio = conflict_ratio\n",
    "        self.dataset = dataset\n",
    "        self.pretrained_path = os.path.join(pretrained_path, 'tag2text', 'tag2text_swin_14m.pth')\n",
    "        self.image_size = image_size\n",
    "        self.tag2text_thres = tag2text_thres\n",
    "        self.device = torch.device(f'cuda:{str(gpu_num)}' if torch.cuda.is_available() else 'cpu')\n",
    "        self.tag2text_model = None\n",
    "        self.class_name = class_name\n",
    "\n",
    "    def load_model(self):\n",
    "        self.tag2text_model = tag2text(pretrained=self.pretrained_path,\n",
    "                                       image_size=self.image_size,\n",
    "                                       vit='swin_b')\n",
    "        self.tag2text_model.thres = self.tag2text_thres  # thres for tagging\n",
    "        self.tag2text_model.eval()\n",
    "        self.tag2text_model = self.tag2text_model.to(self.device)\n",
    "        print(f\"Tag2Text has been loaded. Device: {self.device}\")\n",
    "\n",
    "    def off_model(self):\n",
    "        del self.tag2text_model\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        self.tag2text_model = None\n",
    "\n",
    "    def generate_tag_json(self):\n",
    "        # Load tag2text.\n",
    "        # if self.tag2text_model == None: self.load_model()\n",
    "\n",
    "        # Generate tags.json.\n",
    "        transform = get_transform(dataset=self.dataset,\n",
    "                                  image_size=self.image_size)\n",
    "        \n",
    "        # Inference tags and caption.\n",
    "        path1 = '/mnt/sdc/Debiasing/benchmarks/bffhq/0.5pct/align/0/62242_0_0.png'\n",
    "        path2 = '/mnt/sdc/Debiasing/benchmarks/bffhq/0.5pct/align/0/62387_0_0.png'\n",
    "        image1 = transform(Image.open(path1)).unsqueeze(0).to(self.device)\n",
    "        image2 = transform(Image.open(path2)).unsqueeze(0).to(self.device)\n",
    "        return image1, image2\n",
    "        # res = inference(image, self.tag2text_model)\n",
    "        \n",
    "        \n",
    "bias2tag = Bias2Tag(gpu_num=6,\n",
    "                    dataset='bffhq',\n",
    "                    conflict_ratio='0.5',\n",
    "                    class_name={\n",
    "                                '0': 'young person',\n",
    "                                '1': 'old person'},\n",
    "                    root_path='/mnt/sdc/Debiaisng',\n",
    "                    pretrained_path='/mnt/sdc/Debiasing/pretrained',\n",
    "                    tag2text_thres=0.68)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = bias2tag.generate_tag_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 224, 224])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.concat([a,b]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/encoder/layer/0/crossattention/self/query is tied\n",
      "/encoder/layer/0/crossattention/self/key is tied\n",
      "/encoder/layer/0/crossattention/self/value is tied\n",
      "/encoder/layer/0/crossattention/output/dense is tied\n",
      "/encoder/layer/0/crossattention/output/LayerNorm is tied\n",
      "/encoder/layer/0/intermediate/dense is tied\n",
      "/encoder/layer/0/output/dense is tied\n",
      "/encoder/layer/0/output/LayerNorm is tied\n",
      "/encoder/layer/1/crossattention/self/query is tied\n",
      "/encoder/layer/1/crossattention/self/key is tied\n",
      "/encoder/layer/1/crossattention/self/value is tied\n",
      "/encoder/layer/1/crossattention/output/dense is tied\n",
      "/encoder/layer/1/crossattention/output/LayerNorm is tied\n",
      "/encoder/layer/1/intermediate/dense is tied\n",
      "/encoder/layer/1/output/dense is tied\n",
      "/encoder/layer/1/output/LayerNorm is tied\n",
      "--------------\n",
      "/mnt/sdc/Debiasing/pretrained/tag2text/tag2text_swin_14m.pth\n",
      "--------------\n",
      "Position interpolate visual_encoder.layers.0.blocks.0.attn.relative_position_bias_table from 23x23 to 13x13\n",
      "Position interpolate visual_encoder.layers.0.blocks.1.attn.relative_position_bias_table from 23x23 to 13x13\n",
      "Position interpolate visual_encoder.layers.1.blocks.0.attn.relative_position_bias_table from 23x23 to 13x13\n",
      "Position interpolate visual_encoder.layers.1.blocks.1.attn.relative_position_bias_table from 23x23 to 13x13\n",
      "Position interpolate visual_encoder.layers.2.blocks.0.attn.relative_position_bias_table from 23x23 to 13x13\n",
      "Position interpolate visual_encoder.layers.2.blocks.1.attn.relative_position_bias_table from 23x23 to 13x13\n",
      "Position interpolate visual_encoder.layers.2.blocks.2.attn.relative_position_bias_table from 23x23 to 13x13\n",
      "Position interpolate visual_encoder.layers.2.blocks.3.attn.relative_position_bias_table from 23x23 to 13x13\n",
      "Position interpolate visual_encoder.layers.2.blocks.4.attn.relative_position_bias_table from 23x23 to 13x13\n",
      "Position interpolate visual_encoder.layers.2.blocks.5.attn.relative_position_bias_table from 23x23 to 13x13\n",
      "Position interpolate visual_encoder.layers.2.blocks.6.attn.relative_position_bias_table from 23x23 to 13x13\n",
      "Position interpolate visual_encoder.layers.2.blocks.7.attn.relative_position_bias_table from 23x23 to 13x13\n",
      "Position interpolate visual_encoder.layers.2.blocks.8.attn.relative_position_bias_table from 23x23 to 13x13\n",
      "Position interpolate visual_encoder.layers.2.blocks.9.attn.relative_position_bias_table from 23x23 to 13x13\n",
      "Position interpolate visual_encoder.layers.2.blocks.10.attn.relative_position_bias_table from 23x23 to 13x13\n",
      "Position interpolate visual_encoder.layers.2.blocks.11.attn.relative_position_bias_table from 23x23 to 13x13\n",
      "Position interpolate visual_encoder.layers.2.blocks.12.attn.relative_position_bias_table from 23x23 to 13x13\n",
      "Position interpolate visual_encoder.layers.2.blocks.13.attn.relative_position_bias_table from 23x23 to 13x13\n",
      "Position interpolate visual_encoder.layers.2.blocks.14.attn.relative_position_bias_table from 23x23 to 13x13\n",
      "Position interpolate visual_encoder.layers.2.blocks.15.attn.relative_position_bias_table from 23x23 to 13x13\n",
      "Position interpolate visual_encoder.layers.2.blocks.16.attn.relative_position_bias_table from 23x23 to 13x13\n",
      "Position interpolate visual_encoder.layers.2.blocks.17.attn.relative_position_bias_table from 23x23 to 13x13\n",
      "Position interpolate visual_encoder.layers.3.blocks.0.attn.relative_position_bias_table from 23x23 to 13x13\n",
      "Position interpolate visual_encoder.layers.3.blocks.1.attn.relative_position_bias_table from 23x23 to 13x13\n",
      "load checkpoint from /mnt/sdc/Debiasing/pretrained/tag2text/tag2text_swin_14m.pth\n",
      "vit: swin_b\n",
      "Tag2Text has been loaded. Device: cuda:6\n"
     ]
    }
   ],
   "source": [
    "bias2tag.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = inference(b, bias2tag.tag2text_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('song | hat | woman | band | stage | microphone | person | smile | sing | wear | singe | perform',\n",
       " None,\n",
       " 'a woman wearing a hat sings a song with a band performing on stage')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gene",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
